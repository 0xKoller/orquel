{"version":3,"sources":["../src/index.ts","../src/chunker.ts","../src/orquel.ts"],"sourcesContent":["export * from './types.js';\nexport * from './orquel.js';\nexport * from './chunker.js';","import type { Chunk, IngestSource } from './types.js';\nimport { createHash } from 'node:crypto';\n\nexport interface ChunkerOptions {\n  maxChunkSize?: number;\n  overlap?: number;\n  respectMarkdownHeadings?: boolean;\n}\n\nconst DEFAULT_OPTIONS: Required<ChunkerOptions> = {\n  maxChunkSize: 1200,\n  overlap: 150,\n  respectMarkdownHeadings: true,\n};\n\n/**\n * Default text chunking function with intelligent splitting\n * \n * Features:\n * - Configurable chunk size and overlap\n * - Markdown heading awareness \n * - Word boundary preservation\n * - Content deduplication by hash\n * \n * @param text - Input text to chunk\n * @param source - Source document information\n * @param options - Chunking configuration options\n * @returns Array of text chunks with metadata\n * \n * @example\n * ```typescript\n * const chunks = defaultChunker(\n *   '# Title\\nContent here...',\n *   { title: 'My Doc', kind: 'md' },\n *   { maxChunkSize: 1000, overlap: 100 }\n * );\n * ```\n */\nexport function defaultChunker(\n  text: string,\n  source: IngestSource,\n  options: ChunkerOptions = {}\n): Chunk[] {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  \n  // Normalize text: trim whitespace, collapse repeated spaces, preserve code blocks\n  const normalized = normalizeText(text);\n  \n  if (normalized.length <= opts.maxChunkSize) {\n    return [createChunk(normalized, source, 0)];\n  }\n\n  const chunks: Chunk[] = [];\n  let chunkIndex = 0;\n\n  if (opts.respectMarkdownHeadings && source.kind === 'md') {\n    const sections = splitByMarkdownHeadings(normalized);\n    \n    for (const section of sections) {\n      if (section.length <= opts.maxChunkSize) {\n        chunks.push(createChunk(section, source, chunkIndex++));\n      } else {\n        const subChunks = splitTextIntoChunks(section, opts.maxChunkSize, opts.overlap);\n        for (const subChunk of subChunks) {\n          chunks.push(createChunk(subChunk, source, chunkIndex++));\n        }\n      }\n    }\n  } else {\n    const textChunks = splitTextIntoChunks(normalized, opts.maxChunkSize, opts.overlap);\n    for (const chunk of textChunks) {\n      chunks.push(createChunk(chunk, source, chunkIndex++));\n    }\n  }\n\n  return deduplicateChunks(chunks);\n}\n\nfunction normalizeText(text: string): string {\n  return text\n    .trim()\n    .replace(/\\r\\n/g, '\\n')\n    .replace(/[ \\t]+/g, ' ')\n    .replace(/\\n{3,}/g, '\\n\\n');\n}\n\nfunction splitByMarkdownHeadings(text: string): string[] {\n  const sections: string[] = [];\n  const lines = text.split('\\n');\n  let currentSection = '';\n  \n  for (const line of lines) {\n    if (line.match(/^#{1,6}\\s/)) {\n      if (currentSection.trim()) {\n        sections.push(currentSection.trim());\n      }\n      currentSection = line + '\\n';\n    } else {\n      currentSection += line + '\\n';\n    }\n  }\n  \n  if (currentSection.trim()) {\n    sections.push(currentSection.trim());\n  }\n  \n  return sections.filter(s => s.length > 0);\n}\n\nfunction splitTextIntoChunks(text: string, maxSize: number, overlap: number): string[] {\n  if (text.length <= maxSize) {\n    return [text];\n  }\n\n  const chunks: string[] = [];\n  let start = 0;\n\n  while (start < text.length) {\n    let end = start + maxSize;\n    \n    if (end >= text.length) {\n      chunks.push(text.slice(start));\n      break;\n    }\n\n    // Try to break at word boundary\n    const breakPoint = findBreakPoint(text, start, end);\n    chunks.push(text.slice(start, breakPoint));\n    \n    start = Math.max(start + 1, breakPoint - overlap);\n  }\n\n  return chunks;\n}\n\nfunction findBreakPoint(text: string, start: number, end: number): number {\n  // Look for sentence boundary first\n  for (let i = end - 1; i > start + (end - start) * 0.7; i--) {\n    if (text[i] === '.' && text[i + 1] === ' ') {\n      return i + 1;\n    }\n  }\n  \n  // Fall back to word boundary\n  for (let i = end - 1; i > start + (end - start) * 0.5; i--) {\n    if (text[i] === ' ') {\n      return i;\n    }\n  }\n  \n  return end;\n}\n\nfunction createChunk(text: string, source: IngestSource, chunkIndex: number): Chunk {\n  const hash = createHash('sha256').update(text).digest('hex').slice(0, 16);\n  \n  return {\n    id: `${source.title}-${chunkIndex}-${hash}`,\n    text,\n    metadata: {\n      source,\n      chunkIndex,\n      hash,\n    },\n  };\n}\n\nfunction deduplicateChunks(chunks: Chunk[]): Chunk[] {\n  const seen = new Set<string>();\n  return chunks.filter(chunk => {\n    if (seen.has(chunk.metadata.hash)) {\n      return false;\n    }\n    seen.add(chunk.metadata.hash);\n    return true;\n  });\n}","import type {\n  Orquel,\n  OrquelConfig,\n  IngestArgs,\n  QueryOptions,\n  AnswerOptions,\n  Chunk,\n} from './types.js';\nimport { defaultChunker } from './chunker.js';\n\n/**\n * Create a new Orquel instance with the specified configuration\n * \n * @param config - Configuration object specifying adapters and options\n * @returns Configured Orquel instance\n * \n * @example\n * ```typescript\n * import { createOrquel } from '@orquel/core';\n * import { openAIEmbeddings } from '@orquel/embeddings-openai';\n * import { memoryStore } from '@orquel/store-memory';\n * \n * const orq = createOrquel({\n *   embeddings: openAIEmbeddings(),\n *   vector: memoryStore(),\n * });\n * ```\n */\nexport function createOrquel(config: OrquelConfig): Orquel {\n  const chunker = config.chunker || ((text: string) => \n    defaultChunker(text, { title: 'Unknown' })\n  );\n\n  return {\n    async ingest(args: IngestArgs) {\n      const content = typeof args.content === 'string' \n        ? args.content \n        : args.content.toString('utf-8');\n      \n      const chunks = chunker(content);\n      \n      // Update chunks with proper source info\n      const updatedChunks = chunks.map(chunk => ({\n        ...chunk,\n        metadata: {\n          ...chunk.metadata,\n          source: args.source,\n        },\n      }));\n\n      return {\n        sourceId: args.source.title,\n        chunks: updatedChunks,\n      };\n    },\n\n    async index(chunks: Chunk[]) {\n      // Embed all chunks\n      const texts = chunks.map(chunk => chunk.text);\n      const embeddings = await config.embeddings.embed(texts);\n      \n      // Prepare rows for vector store\n      const rows = chunks.map((chunk, i) => ({\n        ...chunk,\n        embedding: embeddings[i]!,\n      }));\n\n      // Index in vector store\n      await config.vector.upsert(rows);\n\n      // Index in lexical store if available\n      if (config.lexical) {\n        await config.lexical.index(chunks);\n      }\n    },\n\n    async query(q: string, opts: QueryOptions = {}) {\n      const { k = 10, hybrid = !!config.lexical, rerank = !!config.reranker } = opts;\n      \n      let results: Array<{ chunk: Chunk; score: number }> = [];\n\n      if (hybrid && config.lexical) {\n        // Hybrid search: combine dense and lexical\n        const [queryEmbedding] = await config.embeddings.embed([q]);\n        const denseResults = await config.vector.searchByVector(queryEmbedding!, k);\n        const lexicalResults = await config.lexical.search(q, k);\n        \n        // Merge and normalize scores\n        results = mergeHybridResults(denseResults, lexicalResults, k);\n      } else {\n        // Dense-only search\n        const [queryEmbedding] = await config.embeddings.embed([q]);\n        results = await config.vector.searchByVector(queryEmbedding!, k);\n      }\n\n      // Apply reranking if available\n      if (rerank && config.reranker && results.length > 0) {\n        const chunks = results.map(r => r.chunk);\n        const rerankedIndices = await config.reranker.rerank(q, chunks);\n        results = rerankedIndices.map(idx => results[idx]!);\n      }\n\n      return { results };\n    },\n\n    async answer(q: string, opts: AnswerOptions = {}) {\n      const { topK = 4 } = opts;\n      \n      if (!config.answerer) {\n        throw new Error('No answerer configured');\n      }\n\n      // Get relevant contexts\n      const { results } = await this.query(q, { k: topK });\n      const contexts = results.map(r => r.chunk);\n\n      // Generate answer\n      const answer = await config.answerer.answer({\n        query: q,\n        contexts,\n      });\n\n      return { answer, contexts };\n    },\n  };\n}\n\nfunction mergeHybridResults(\n  denseResults: Array<{ chunk: Chunk; score: number }>,\n  lexicalResults: Array<{ chunk: Chunk; score: number }>,\n  k: number,\n  denseWeight = 0.65,\n  lexicalWeight = 0.35\n): Array<{ chunk: Chunk; score: number }> {\n  // Normalize scores to [0, 1]\n  const normalizedDense = normalizeScores(denseResults);\n  const normalizedLexical = normalizeScores(lexicalResults);\n\n  // Create a map of chunk ID to combined score\n  const scoreMap = new Map<string, { chunk: Chunk; score: number }>();\n\n  // Add dense results\n  for (const result of normalizedDense) {\n    scoreMap.set(result.chunk.id, {\n      chunk: result.chunk,\n      score: result.score * denseWeight,\n    });\n  }\n\n  // Add/merge lexical results\n  for (const result of normalizedLexical) {\n    const existing = scoreMap.get(result.chunk.id);\n    if (existing) {\n      existing.score += result.score * lexicalWeight;\n    } else {\n      scoreMap.set(result.chunk.id, {\n        chunk: result.chunk,\n        score: result.score * lexicalWeight,\n      });\n    }\n  }\n\n  // Sort by combined score and take top k\n  return Array.from(scoreMap.values())\n    .sort((a, b) => b.score - a.score)\n    .slice(0, k);\n}\n\nfunction normalizeScores(\n  results: Array<{ chunk: Chunk; score: number }>\n): Array<{ chunk: Chunk; score: number }> {\n  if (results.length === 0) return results;\n\n  const scores = results.map(r => r.score);\n  const min = Math.min(...scores);\n  const max = Math.max(...scores);\n  const range = max - min;\n\n  if (range === 0) {\n    return results.map(r => ({ ...r, score: 1 }));\n  }\n\n  return results.map(r => ({\n    ...r,\n    score: (r.score - min) / range,\n  }));\n}"],"mappings":";;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACCA,yBAA2B;AAQ3B,IAAM,kBAA4C;AAAA,EAChD,cAAc;AAAA,EACd,SAAS;AAAA,EACT,yBAAyB;AAC3B;AAyBO,SAAS,eACd,MACA,QACA,UAA0B,CAAC,GAClB;AACT,QAAM,OAAO,EAAE,GAAG,iBAAiB,GAAG,QAAQ;AAG9C,QAAM,aAAa,cAAc,IAAI;AAErC,MAAI,WAAW,UAAU,KAAK,cAAc;AAC1C,WAAO,CAAC,YAAY,YAAY,QAAQ,CAAC,CAAC;AAAA,EAC5C;AAEA,QAAM,SAAkB,CAAC;AACzB,MAAI,aAAa;AAEjB,MAAI,KAAK,2BAA2B,OAAO,SAAS,MAAM;AACxD,UAAM,WAAW,wBAAwB,UAAU;AAEnD,eAAW,WAAW,UAAU;AAC9B,UAAI,QAAQ,UAAU,KAAK,cAAc;AACvC,eAAO,KAAK,YAAY,SAAS,QAAQ,YAAY,CAAC;AAAA,MACxD,OAAO;AACL,cAAM,YAAY,oBAAoB,SAAS,KAAK,cAAc,KAAK,OAAO;AAC9E,mBAAW,YAAY,WAAW;AAChC,iBAAO,KAAK,YAAY,UAAU,QAAQ,YAAY,CAAC;AAAA,QACzD;AAAA,MACF;AAAA,IACF;AAAA,EACF,OAAO;AACL,UAAM,aAAa,oBAAoB,YAAY,KAAK,cAAc,KAAK,OAAO;AAClF,eAAW,SAAS,YAAY;AAC9B,aAAO,KAAK,YAAY,OAAO,QAAQ,YAAY,CAAC;AAAA,IACtD;AAAA,EACF;AAEA,SAAO,kBAAkB,MAAM;AACjC;AAEA,SAAS,cAAc,MAAsB;AAC3C,SAAO,KACJ,KAAK,EACL,QAAQ,SAAS,IAAI,EACrB,QAAQ,WAAW,GAAG,EACtB,QAAQ,WAAW,MAAM;AAC9B;AAEA,SAAS,wBAAwB,MAAwB;AACvD,QAAM,WAAqB,CAAC;AAC5B,QAAM,QAAQ,KAAK,MAAM,IAAI;AAC7B,MAAI,iBAAiB;AAErB,aAAW,QAAQ,OAAO;AACxB,QAAI,KAAK,MAAM,WAAW,GAAG;AAC3B,UAAI,eAAe,KAAK,GAAG;AACzB,iBAAS,KAAK,eAAe,KAAK,CAAC;AAAA,MACrC;AACA,uBAAiB,OAAO;AAAA,IAC1B,OAAO;AACL,wBAAkB,OAAO;AAAA,IAC3B;AAAA,EACF;AAEA,MAAI,eAAe,KAAK,GAAG;AACzB,aAAS,KAAK,eAAe,KAAK,CAAC;AAAA,EACrC;AAEA,SAAO,SAAS,OAAO,OAAK,EAAE,SAAS,CAAC;AAC1C;AAEA,SAAS,oBAAoB,MAAc,SAAiB,SAA2B;AACrF,MAAI,KAAK,UAAU,SAAS;AAC1B,WAAO,CAAC,IAAI;AAAA,EACd;AAEA,QAAM,SAAmB,CAAC;AAC1B,MAAI,QAAQ;AAEZ,SAAO,QAAQ,KAAK,QAAQ;AAC1B,QAAI,MAAM,QAAQ;AAElB,QAAI,OAAO,KAAK,QAAQ;AACtB,aAAO,KAAK,KAAK,MAAM,KAAK,CAAC;AAC7B;AAAA,IACF;AAGA,UAAM,aAAa,eAAe,MAAM,OAAO,GAAG;AAClD,WAAO,KAAK,KAAK,MAAM,OAAO,UAAU,CAAC;AAEzC,YAAQ,KAAK,IAAI,QAAQ,GAAG,aAAa,OAAO;AAAA,EAClD;AAEA,SAAO;AACT;AAEA,SAAS,eAAe,MAAc,OAAe,KAAqB;AAExE,WAAS,IAAI,MAAM,GAAG,IAAI,SAAS,MAAM,SAAS,KAAK,KAAK;AAC1D,QAAI,KAAK,CAAC,MAAM,OAAO,KAAK,IAAI,CAAC,MAAM,KAAK;AAC1C,aAAO,IAAI;AAAA,IACb;AAAA,EACF;AAGA,WAAS,IAAI,MAAM,GAAG,IAAI,SAAS,MAAM,SAAS,KAAK,KAAK;AAC1D,QAAI,KAAK,CAAC,MAAM,KAAK;AACnB,aAAO;AAAA,IACT;AAAA,EACF;AAEA,SAAO;AACT;AAEA,SAAS,YAAY,MAAc,QAAsB,YAA2B;AAClF,QAAM,WAAO,+BAAW,QAAQ,EAAE,OAAO,IAAI,EAAE,OAAO,KAAK,EAAE,MAAM,GAAG,EAAE;AAExE,SAAO;AAAA,IACL,IAAI,GAAG,OAAO,KAAK,IAAI,UAAU,IAAI,IAAI;AAAA,IACzC;AAAA,IACA,UAAU;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AACF;AAEA,SAAS,kBAAkB,QAA0B;AACnD,QAAM,OAAO,oBAAI,IAAY;AAC7B,SAAO,OAAO,OAAO,WAAS;AAC5B,QAAI,KAAK,IAAI,MAAM,SAAS,IAAI,GAAG;AACjC,aAAO;AAAA,IACT;AACA,SAAK,IAAI,MAAM,SAAS,IAAI;AAC5B,WAAO;AAAA,EACT,CAAC;AACH;;;ACpJO,SAAS,aAAa,QAA8B;AACzD,QAAM,UAAU,OAAO,YAAY,CAAC,SAClC,eAAe,MAAM,EAAE,OAAO,UAAU,CAAC;AAG3C,SAAO;AAAA,IACL,MAAM,OAAO,MAAkB;AAC7B,YAAM,UAAU,OAAO,KAAK,YAAY,WACpC,KAAK,UACL,KAAK,QAAQ,SAAS,OAAO;AAEjC,YAAM,SAAS,QAAQ,OAAO;AAG9B,YAAM,gBAAgB,OAAO,IAAI,YAAU;AAAA,QACzC,GAAG;AAAA,QACH,UAAU;AAAA,UACR,GAAG,MAAM;AAAA,UACT,QAAQ,KAAK;AAAA,QACf;AAAA,MACF,EAAE;AAEF,aAAO;AAAA,QACL,UAAU,KAAK,OAAO;AAAA,QACtB,QAAQ;AAAA,MACV;AAAA,IACF;AAAA,IAEA,MAAM,MAAM,QAAiB;AAE3B,YAAM,QAAQ,OAAO,IAAI,WAAS,MAAM,IAAI;AAC5C,YAAM,aAAa,MAAM,OAAO,WAAW,MAAM,KAAK;AAGtD,YAAM,OAAO,OAAO,IAAI,CAAC,OAAO,OAAO;AAAA,QACrC,GAAG;AAAA,QACH,WAAW,WAAW,CAAC;AAAA,MACzB,EAAE;AAGF,YAAM,OAAO,OAAO,OAAO,IAAI;AAG/B,UAAI,OAAO,SAAS;AAClB,cAAM,OAAO,QAAQ,MAAM,MAAM;AAAA,MACnC;AAAA,IACF;AAAA,IAEA,MAAM,MAAM,GAAW,OAAqB,CAAC,GAAG;AAC9C,YAAM,EAAE,IAAI,IAAI,SAAS,CAAC,CAAC,OAAO,SAAS,SAAS,CAAC,CAAC,OAAO,SAAS,IAAI;AAE1E,UAAI,UAAkD,CAAC;AAEvD,UAAI,UAAU,OAAO,SAAS;AAE5B,cAAM,CAAC,cAAc,IAAI,MAAM,OAAO,WAAW,MAAM,CAAC,CAAC,CAAC;AAC1D,cAAM,eAAe,MAAM,OAAO,OAAO,eAAe,gBAAiB,CAAC;AAC1E,cAAM,iBAAiB,MAAM,OAAO,QAAQ,OAAO,GAAG,CAAC;AAGvD,kBAAU,mBAAmB,cAAc,gBAAgB,CAAC;AAAA,MAC9D,OAAO;AAEL,cAAM,CAAC,cAAc,IAAI,MAAM,OAAO,WAAW,MAAM,CAAC,CAAC,CAAC;AAC1D,kBAAU,MAAM,OAAO,OAAO,eAAe,gBAAiB,CAAC;AAAA,MACjE;AAGA,UAAI,UAAU,OAAO,YAAY,QAAQ,SAAS,GAAG;AACnD,cAAM,SAAS,QAAQ,IAAI,OAAK,EAAE,KAAK;AACvC,cAAM,kBAAkB,MAAM,OAAO,SAAS,OAAO,GAAG,MAAM;AAC9D,kBAAU,gBAAgB,IAAI,SAAO,QAAQ,GAAG,CAAE;AAAA,MACpD;AAEA,aAAO,EAAE,QAAQ;AAAA,IACnB;AAAA,IAEA,MAAM,OAAO,GAAW,OAAsB,CAAC,GAAG;AAChD,YAAM,EAAE,OAAO,EAAE,IAAI;AAErB,UAAI,CAAC,OAAO,UAAU;AACpB,cAAM,IAAI,MAAM,wBAAwB;AAAA,MAC1C;AAGA,YAAM,EAAE,QAAQ,IAAI,MAAM,KAAK,MAAM,GAAG,EAAE,GAAG,KAAK,CAAC;AACnD,YAAM,WAAW,QAAQ,IAAI,OAAK,EAAE,KAAK;AAGzC,YAAM,SAAS,MAAM,OAAO,SAAS,OAAO;AAAA,QAC1C,OAAO;AAAA,QACP;AAAA,MACF,CAAC;AAED,aAAO,EAAE,QAAQ,SAAS;AAAA,IAC5B;AAAA,EACF;AACF;AAEA,SAAS,mBACP,cACA,gBACA,GACA,cAAc,MACd,gBAAgB,MACwB;AAExC,QAAM,kBAAkB,gBAAgB,YAAY;AACpD,QAAM,oBAAoB,gBAAgB,cAAc;AAGxD,QAAM,WAAW,oBAAI,IAA6C;AAGlE,aAAW,UAAU,iBAAiB;AACpC,aAAS,IAAI,OAAO,MAAM,IAAI;AAAA,MAC5B,OAAO,OAAO;AAAA,MACd,OAAO,OAAO,QAAQ;AAAA,IACxB,CAAC;AAAA,EACH;AAGA,aAAW,UAAU,mBAAmB;AACtC,UAAM,WAAW,SAAS,IAAI,OAAO,MAAM,EAAE;AAC7C,QAAI,UAAU;AACZ,eAAS,SAAS,OAAO,QAAQ;AAAA,IACnC,OAAO;AACL,eAAS,IAAI,OAAO,MAAM,IAAI;AAAA,QAC5B,OAAO,OAAO;AAAA,QACd,OAAO,OAAO,QAAQ;AAAA,MACxB,CAAC;AAAA,IACH;AAAA,EACF;AAGA,SAAO,MAAM,KAAK,SAAS,OAAO,CAAC,EAChC,KAAK,CAAC,GAAG,MAAM,EAAE,QAAQ,EAAE,KAAK,EAChC,MAAM,GAAG,CAAC;AACf;AAEA,SAAS,gBACP,SACwC;AACxC,MAAI,QAAQ,WAAW,EAAG,QAAO;AAEjC,QAAM,SAAS,QAAQ,IAAI,OAAK,EAAE,KAAK;AACvC,QAAM,MAAM,KAAK,IAAI,GAAG,MAAM;AAC9B,QAAM,MAAM,KAAK,IAAI,GAAG,MAAM;AAC9B,QAAM,QAAQ,MAAM;AAEpB,MAAI,UAAU,GAAG;AACf,WAAO,QAAQ,IAAI,QAAM,EAAE,GAAG,GAAG,OAAO,EAAE,EAAE;AAAA,EAC9C;AAEA,SAAO,QAAQ,IAAI,QAAM;AAAA,IACvB,GAAG;AAAA,IACH,QAAQ,EAAE,QAAQ,OAAO;AAAA,EAC3B,EAAE;AACJ;","names":[]}
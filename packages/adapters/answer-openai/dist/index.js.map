{"version":3,"sources":["../src/index.ts"],"sourcesContent":["import type { AnswerAdapter, Chunk } from '@orquel/core';\nimport OpenAI from 'openai';\n\nexport interface OpenAIAnswererOptions {\n  apiKey?: string;\n  model?: 'gpt-4' | 'gpt-4-turbo' | 'gpt-3.5-turbo';\n  temperature?: number;\n  maxTokens?: number;\n  maxRetries?: number;\n  systemPrompt?: string;\n}\n\nconst DEFAULT_SYSTEM_PROMPT = `You are a helpful assistant that answers questions based on the provided context. \nFollow these guidelines:\n- Answer based only on the provided context\n- If the context doesn't contain enough information, say so\n- Be concise but comprehensive\n- Use the same language as the question\n- Include relevant details from the context`;\n\nexport function openAIAnswerer(options: OpenAIAnswererOptions = {}): AnswerAdapter {\n  const {\n    apiKey = process.env.OPENAI_API_KEY,\n    model = 'gpt-4',\n    temperature = 0.1,\n    maxTokens = 1000,\n    maxRetries = 3,\n    systemPrompt = DEFAULT_SYSTEM_PROMPT,\n  } = options;\n\n  if (!apiKey) {\n    throw new Error(\n      'OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass apiKey option.'\n    );\n  }\n\n  const openai = new OpenAI({\n    apiKey,\n    maxRetries,\n  });\n\n  return {\n    name: `openai-${model}`,\n\n    async answer(args: { query: string; contexts: Chunk[] }): Promise<string> {\n      const { query, contexts } = args;\n\n      if (contexts.length === 0) {\n        return \"I don't have enough context to answer your question.\";\n      }\n\n      // Format contexts for the prompt\n      const contextText = contexts\n        .map((chunk, i) => `[${i + 1}] ${chunk.text.trim()}`)\n        .join('\\n\\n');\n\n      const userPrompt = `Context:\n${contextText}\n\nQuestion: ${query}\n\nAnswer:`;\n\n      try {\n        const response = await openai.chat.completions.create({\n          model,\n          messages: [\n            { role: 'system', content: systemPrompt },\n            { role: 'user', content: userPrompt },\n          ],\n          temperature,\n          max_tokens: maxTokens,\n          stream: false,\n        });\n\n        const answer = response.choices[0]?.message?.content?.trim();\n        \n        if (!answer) {\n          throw new Error('No answer generated');\n        }\n\n        return answer;\n      } catch (error) {\n        throw new Error(\n          `Failed to generate answer: ${error instanceof Error ? error.message : 'Unknown error'}`\n        );\n      }\n    },\n  };\n}"],"mappings":";AACA,OAAO,YAAY;AAWnB,IAAM,wBAAwB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQvB,SAAS,eAAe,UAAiC,CAAC,GAAkB;AACjF,QAAM;AAAA,IACJ,SAAS,QAAQ,IAAI;AAAA,IACrB,QAAQ;AAAA,IACR,cAAc;AAAA,IACd,YAAY;AAAA,IACZ,aAAa;AAAA,IACb,eAAe;AAAA,EACjB,IAAI;AAEJ,MAAI,CAAC,QAAQ;AACX,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,QAAM,SAAS,IAAI,OAAO;AAAA,IACxB;AAAA,IACA;AAAA,EACF,CAAC;AAED,SAAO;AAAA,IACL,MAAM,UAAU,KAAK;AAAA,IAErB,MAAM,OAAO,MAA6D;AACxE,YAAM,EAAE,OAAO,SAAS,IAAI;AAE5B,UAAI,SAAS,WAAW,GAAG;AACzB,eAAO;AAAA,MACT;AAGA,YAAM,cAAc,SACjB,IAAI,CAAC,OAAO,MAAM,IAAI,IAAI,CAAC,KAAK,MAAM,KAAK,KAAK,CAAC,EAAE,EACnD,KAAK,MAAM;AAEd,YAAM,aAAa;AAAA,EACvB,WAAW;AAAA;AAAA,YAED,KAAK;AAAA;AAAA;AAIX,UAAI;AACF,cAAM,WAAW,MAAM,OAAO,KAAK,YAAY,OAAO;AAAA,UACpD;AAAA,UACA,UAAU;AAAA,YACR,EAAE,MAAM,UAAU,SAAS,aAAa;AAAA,YACxC,EAAE,MAAM,QAAQ,SAAS,WAAW;AAAA,UACtC;AAAA,UACA;AAAA,UACA,YAAY;AAAA,UACZ,QAAQ;AAAA,QACV,CAAC;AAED,cAAM,SAAS,SAAS,QAAQ,CAAC,GAAG,SAAS,SAAS,KAAK;AAE3D,YAAI,CAAC,QAAQ;AACX,gBAAM,IAAI,MAAM,qBAAqB;AAAA,QACvC;AAEA,eAAO;AAAA,MACT,SAAS,OAAO;AACd,cAAM,IAAI;AAAA,UACR,8BAA8B,iBAAiB,QAAQ,MAAM,UAAU,eAAe;AAAA,QACxF;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;","names":[]}